{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC-LZ20S_WUr"
   },
   "source": [
    "# Assignment 2: Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9xqCFJBv_WUt"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq8sr9x17KhU"
   },
   "source": [
    "## Task 1: Named Entity Annotation (10 Marks)\n",
    "\n",
    "Using the IOB tagging scheme annotate all of the named entities (PERson, LOCation, ORGanisation, TIME) in the following sentence:\n",
    "\n",
    "*Wayne Rooney is a professional footballer from England who last played for Major League Soccer club D.C. United and will join Derby County in January 2020.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htlSW1ad81D-"
   },
   "source": [
    "Edit this cell and write your annotation below the line. (Note that you don't have to write code for this task, you have to annotate it manually)\n",
    "\n",
    "---\n",
    "B_PER:Wayne I_PER:Rooney O:is O:a O:professional O:footballer O:from B_LOC:England O:who O:last O:played O:for B_ORG:Major I_ORG:League I_ORG:Soccer I_ORG:club I_ORG:D.C I_ORG:United O:and O:will O:join B_ORG:Derby I_ORG:County O:in B_TIME:January I_TIME:2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZNmDWxj-V-J"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### For subsequent tasks in this assignment, you will work with the documents in `football_players.txt` to perform various information extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V9YE4n6u7olU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 24172  100 24172    0     0  30871      0 --:--:-- --:--:-- --:--:-- 30871\n"
     ]
    }
   ],
   "source": [
    "# Download the text file (uncomment the line below in this cell, if not already downloaded from Blackboard)\n",
    "!curl \"https://ideone.com/plain/OvwDXZ\" > football_players.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpSaij2b73Vj"
   },
   "source": [
    " Read all the documents from `football_players.txt` into a list called `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qteh89cs7q4x"
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "# your code goes here\n",
    "line = open('football_players.txt', 'r').read()\n",
    "docs = line.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCEJrJ-p_WU1"
   },
   "source": [
    "## Task 2 (10 Marks)\n",
    "Write a function that takes a document and returns a list of sentences with part-of-speech tags.\n",
    "\n",
    "Please keep in mind that the expected output is a list within a list as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO7Fyfq7DYxW"
   },
   "source": [
    "Hint: For this task you need to perform three steps:\n",
    "1. Sentence Segmentation\n",
    "1. Word Tokenization\n",
    "1. Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U3MCJIcR_WU2"
   },
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "  # your code goes here\n",
    "  \n",
    "    # Step 1: Sentence segmentation.\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "\n",
    "    # Step 2: Tokenize sentences into words.\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # Step 3: POS tagging.\n",
    "    tagged_sentences = [nltk.pos_tag(sent) for sent in tokenized_sentences]\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E04CUNb_WU6"
   },
   "source": [
    "Run the cell below to verify your result for the second sentence in the first document.\n",
    "Expected output: \n",
    "`[('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('forward', 'NN'), ('and', 'CC'), ('serves', 'NNS'), ('as', 'IN'), ('captain', 'NN'), ('for', 'IN'), ('Portugal', 'NNP'), ('.', '.')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R30taRgf_WU6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('forward', 'NN'), ('and', 'CC'), ('serves', 'NNS'), ('as', 'IN'), ('captain', 'NN'), ('for', 'IN'), ('Portugal', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "first_doc = docs[0]\n",
    "tagged_sentences = ie_preprocess(first_doc)\n",
    "print(tagged_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYTwrZId_WU_"
   },
   "source": [
    "## Task 3 (20 Marks)\n",
    "Write a function that takes a list of tokens with POS tags for a sentence and returns a list of named entities (NE). \n",
    "\n",
    "Hint: Use `binary = True` while calling NE chunk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "id": "5fC0iqJJ_WU_"
   },
   "outputs": [],
   "source": [
    "def find_named_entities(sent):\n",
    "    named_entities = []\n",
    "  # your code goes here    \n",
    "    tree = nltk.ne_chunk(sent, binary = True)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NE':\n",
    "            entity = \"\"\n",
    "            for leaf in subtree.leaves():\n",
    "              entity = entity + leaf[0] + \" \"\n",
    "            named_entities.append(entity.strip())\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td5yJ8cgFScx"
   },
   "source": [
    "Run the cell below to verify your result for the first sentence in the first document.\n",
    "Expected output: `['Cristiano Ronaldo', 'Santos Aveiro', 'ComM', 'GOIH', 'Portuguese', 'Portuguese', 'Spanish', 'Real Madrid', 'Portugal']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "id": "FijjdAPWFsp2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cristiano Ronaldo',\n",
       " 'Santos Aveiro',\n",
       " 'ComM',\n",
       " 'GOIH',\n",
       " 'Portuguese',\n",
       " 'Portuguese',\n",
       " 'Spanish',\n",
       " 'Real Madrid',\n",
       " 'Portugal']"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences = ie_preprocess(docs[0])\n",
    "\n",
    "find_named_entities(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHMp7xtK_WVE"
   },
   "source": [
    "## Task 4 (5 Marks)\n",
    "\n",
    "Implement the `find_all_named_entities` function below to find **all** NEs in a given document.\n",
    "\n",
    "Hint: Use `find_named_entities` implemented above for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "TwFlzQx4_WVF"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def find_all_named_entities(doc):\n",
    "    named_entities = []\n",
    "    named_entities_temp = []\n",
    "  # your code goes here\n",
    "    odc_sent = doc.split(\".\")\n",
    "    tagged_sentences = ie_preprocess(doc)\n",
    "    for i in range(len(tagged_sentences)):\n",
    "        named_entities_temp.append(find_named_entities(tagged_sentences[i]))\n",
    "    named_entities = list(itertools.chain.from_iterable(named_entities_temp))  \n",
    "    return named_entities   # return a flat list and not a list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdM0-LZlJy4u"
   },
   "source": [
    "How many named entities did you find in the first document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "id": "_ajmnnOqJ8V6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "len(find_all_named_entities(docs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2AzD9MVNIx2"
   },
   "source": [
    "## Task 5 (5 Marks)\n",
    "\n",
    "Find named entities across **all** documents in `football_players.txt`, and save the result into a single flat list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "id": "YULMcK1-NSR9"
   },
   "outputs": [],
   "source": [
    "all_named_entities = []\n",
    "# your code goes here\n",
    "for i in docs:\n",
    "    ne = find_all_named_entities(i)\n",
    "    for i in ne:\n",
    "        all_named_entities.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaM9Cs9zNGM2"
   },
   "source": [
    "How many named entities did you find across all documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "id": "jCNIrC_SNpHQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "len(all_named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7-ma9lJ_WVJ"
   },
   "source": [
    "## Task 6 (40 Marks)\n",
    "\n",
    "Write functions to extract the name of the player, country of origin and date of birth as well as the following relations: team(s) of the player and position(s) of the player.\n",
    "\n",
    "Hint: Use the `re.compile()` function to create the extraction patterns.\n",
    "\n",
    "Reference: https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2]]\n",
    "a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tbaFyiah_WVK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cristiano Ronaldo dos Santos Aveiro\n",
      "Portugal\n",
      "['Spanish club Real Madrid', 'the Portugal national team']\n",
      "5-February-1985\n",
      "['forward']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\"\"\"\n",
    "Taking the required sentence through regex by filtering on 'is a or is an' and then checking through the grammer \n",
    "for the required pattern and also to cross validate the answer by checking the specified grammer.\n",
    "\"\"\"\n",
    "def name_of_the_player(doc):\n",
    "  # your code goes here\n",
    "    pattern = (r'.*?(\\bis a| is an \\b).+?')\n",
    "    match = re.search(pattern, doc)\n",
    "    text = match.group(0)\n",
    "    grammar =  r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*>*<``|''|NNP|FW>*<NN.*|NNP.*>}  # starting with noun only, having NNP or FW in the middle and ending with either NN or NNP only.\n",
    "      \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tokenized_sentence = nltk.word_tokenize(text)\n",
    "    tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "    tree = chunker.parse(ie_preprocess(text)[0])\n",
    "    leave = []\n",
    "    for subtree in tree.subtrees():\n",
    "        leave.append(subtree.leaves())\n",
    "    res = \"\"\n",
    "    for i in range(len(leave[1])):\n",
    "        res += \" \" +leave[1][i][0]\n",
    "    return res\n",
    "\n",
    "\"\"\"\n",
    "Taking the required sentence through regex by filtering on 'national team' and then checking through the grammer \n",
    "for the required pattern and also to cross validate the answer by checking the specified grammer.\n",
    "\"\"\"\n",
    "\n",
    "def country_of_origin(doc):\n",
    "  # your code goes here\n",
    "    pattern =  (r'.*?(\\bnational team\\b).+?')\n",
    "    match = re.search(pattern, doc)\n",
    "    text = match.group(0)\n",
    "    t = text.strip().split(\" \")\n",
    "    grammar =  r\"\"\"\n",
    "    NBAR:\n",
    "        {<JJ>|<NNP>}  # Only NNP or JJ words\n",
    "  \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tokenized_sentence = nltk.word_tokenize(text)\n",
    "    tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "    tree = chunker.parse(tagged_sentence)\n",
    "    leave = []\n",
    "    for subtree in tree.subtrees():\n",
    "        leave.append(subtree.leaves())\n",
    "    return t[-3]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Taking the required sentence through regex by filtering on 'born' and then checking through the grammer \n",
    "for the required pattern and also to cross validate the answer by checking the specified grammer.\n",
    "\"\"\"\n",
    "\n",
    "def date_of_birth(doc):\n",
    "  # your code goes here\n",
    "    pattern =  (r'.?(\\bborn \\b).*\\.')\n",
    "    match = re.search(pattern, doc)\n",
    "    text = match.group(0)\n",
    "    grammar =  r\"\"\"\n",
    "    NBAR:\n",
    "        {<CD><NNP><CD>}  # CD for Day, NNP for month and CD for Year\n",
    "   \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tokenized_sentence = nltk.word_tokenize(text)\n",
    "    tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "    tree = chunker.parse(tagged_sentence)\n",
    "    leave = []\n",
    "    for subtree in tree.subtrees():\n",
    "        leave.append(subtree.leaves())\n",
    "    date = \"\"\n",
    "    for i in range(len(leave[1])):\n",
    "        if(i == 0):\n",
    "            date += leave[1][i][0] \n",
    "        else:\n",
    "            date += \"-\"+leave[1][i][0]\n",
    "    return date\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Taking the required sentence through regex by filtering on 'for' and then checking through the grammer \n",
    "for the required pattern and also to cross validate the answer by checking the specified grammer.\n",
    "\"\"\"\n",
    "def team_of_the_player(doc):\n",
    "  # your code goes here\n",
    "    pattern =  (r'.?(\\bfor \\b).*\\.')\n",
    "    match = re.search(pattern, doc)\n",
    "    text = match.group(0)\n",
    "    grammar =  r\"\"\"\n",
    "    NBAR:\n",
    "        {<JJ|NNP><NN|NNN|NNP|CC|DT|JJ|,>*<NN|NNP>}  # starting JJ or NNP, after that it could be NN or NNN or NNP or CC or DT or JJ, and ending with either NN or NNP\n",
    "   \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tokenized_sentence = nltk.word_tokenize(text)\n",
    "    tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "    tree = chunker.parse(tagged_sentence)\n",
    "    leave = []\n",
    "    for subtree in tree.subtrees():\n",
    "        leave.append(subtree.leaves())\n",
    "    team = \"\"\n",
    "    for i in range(len(leave[1])):\n",
    "        if(i == 0):\n",
    "            team += leave[1][i][0] \n",
    "        else:\n",
    "            team += \" \"+leave[1][i][0]\n",
    "    team = re.split(' and |[,]', team)\n",
    "    team = [i.strip() for i in team]\n",
    "    return team\n",
    "\n",
    "\"\"\"\n",
    "As their finite number of positions in football we can filter on list of the positions and check if it matches in the document.\n",
    "After finding a position I am checking through the grammer \n",
    "for the required pattern and also to cross validate the answer by checking the specified grammer.\n",
    "\"\"\"\n",
    "\n",
    "def position_of_the_player(doc):\n",
    "  # your code goes here\n",
    "    pattern = (r'.?(\\b winger.*|central midfielder.*|attacking midfielder.*|forward.*|striker.*|right winger.*|left back.*\\b).*\\.')\n",
    "    match = re.search(pattern, doc)\n",
    "    text = match.group(0)\n",
    "    grammar =  r\"\"\"\n",
    "    NBAR:\n",
    "        {<RB|VBG|NN|JJ>*<NN|CC|RB>*<RB|NN>}  # starting with RB or VBG or NN or JJ and multiple occurences and ending with maybe RB or NN\n",
    "       \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tokenized_sentence = nltk.word_tokenize(text)\n",
    "    tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "    tree = chunker.parse(tagged_sentence)\n",
    "    leave = []\n",
    "    for subtree in tree.subtrees():\n",
    "        leave.append(subtree.leaves())\n",
    "    position = \"\"\n",
    "    for i in range(len(leave[1])):\n",
    "        if(i == 0):\n",
    "            position += leave[1][i][0] \n",
    "        else:\n",
    "            position += \" \"+leave[1][i][0]\n",
    "    position = re.split(' or | and ', position)\n",
    "    return position\n",
    "\n",
    "\n",
    "print(name_of_the_player(docs[0]))\n",
    "print(country_of_origin(docs[0]))\n",
    "print(team_of_the_player(docs[0]))\n",
    "print(date_of_birth(docs[0]))\n",
    "print(position_of_the_player(docs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-CNrMM5_WVO"
   },
   "source": [
    "Execute the cell below to verify the `date_of_birth` function for the third player. Expected output `5 February 1992`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "id": "jpeKE1u9_WVP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5-February-1992'"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_of_birth(docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3VtWxBr_WVZ"
   },
   "source": [
    "## Task 6 (10 Marks)\n",
    "Identify one other relation (besides team and player) and write a function to extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "id": "TR0GZrUB_WVa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cristiano Ronaldo dos Santos Aveiro  -  [\"first Ballon d'Or\", \"FIFA World Player of the Year awards the FIFA Ballon d'Or in 2013\", '2014 one La Liga title', 'two Copas del Rey', 'two Champions League titles', '', 'a Club World Cup']\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\"\"\" \n",
    "For this task I am looking for all the awards the player has won through out the document. \n",
    "We can find all the sentences that have won in them and can look through grammer for the awards which are mostly starting from JJ.\n",
    "Printing it with the name of the player for quick understanding. But there are case where a player might not have\n",
    "won any awards. This condition is also being handled in the code below.\n",
    "\"\"\"\n",
    "\n",
    "def awards_of_the_player(doc):\n",
    "  # your code goes here\n",
    "    pattern = re.compile(r'(?:\\bwon)[\\w+\\s+,-]+[A-Z]+[^\\.]+')\n",
    "    match = re.search(pattern, doc)\n",
    "    text = pattern.findall(doc)\n",
    "    tagged_sentence = []\n",
    "    grammar =  r\"\"\"\n",
    "    NBAR:\n",
    "        {<JJ|NNP|CD>.*<.*>*}  # starting with JJ or NNP or CD with multiple occurences and could end with anything.\n",
    "       \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    for sub_text in text:\n",
    "        tokenized_sent = nltk.word_tokenize(sub_text)\n",
    "        tagged_sentence.append(nltk.pos_tag(tokenized_sent))\n",
    "    tagged_sent = list(itertools.chain.from_iterable(tagged_sentence))\n",
    "    if len(tagged_sent) > 0:\n",
    "        tree = chunker.parse(tagged_sent)\n",
    "        leave = []\n",
    "        for subtree in tree.subtrees():\n",
    "            leave.append(subtree.leaves())\n",
    "        award = \"\"\n",
    "        for i in range(len(leave[1])):\n",
    "            if(leave[1][i][0] != 'won'):\n",
    "                if(i == 0):\n",
    "                    award += leave[1][i][0] \n",
    "                else:\n",
    "                    award += \" \"+leave[1][i][0]\n",
    "        award = re.split(' and |[,]', award)\n",
    "        award = [i.strip() for i in award]\n",
    "        return award\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "print(name_of_the_player(docs[0]), \" - \", awards_of_the_player(docs[0]))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 2.ipynb",
   "provenance": [
    {
     "file_id": "1EXXdimBbQY8nnqIs5hBXdG68r2hsk7HS",
     "timestamp": 1604940588321
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
